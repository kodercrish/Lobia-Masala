{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef65d78e-07c3-4919-b2cf-cfc5b7894d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading and Initial Setup ---\n",
      "Submission file saved as 'random_forest_submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# #try random forest\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# # Define file paths. Assuming files are in the current working directory.\n",
    "# TARGET_COLUMN = 'HotelValue'\n",
    "# train_data_path = 'train.csv'\n",
    "# test_data_path = 'test.csv'\n",
    "\n",
    "\n",
    "# # --- 1. Data Loading and Initial Setup ---\n",
    "# print(\"--- 1. Data Loading and Initial Setup ---\")\n",
    "# # Attempt to load files (using a robust method to handle potential path changes)\n",
    "# try:\n",
    "#     df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "#     df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "# except FileNotFoundError:\n",
    "#     df_train = pd.read_csv(train_data_path)\n",
    "#     df_test = pd.read_csv(test_data_path)\n",
    "\n",
    "# X_train = df_train.drop(columns=[TARGET_COLUMN])\n",
    "# y_train = df_train[TARGET_COLUMN]\n",
    "# X_test = df_test.copy()\n",
    "\n",
    "# test_ids = X_test['Id']\n",
    "\n",
    "# # Columns to drop due to irrelevance or excessive missing values\n",
    "# columns_to_drop = [\n",
    "#     'Id',\n",
    "#     'PoolQuality',       # Excessive missing data\n",
    "#     'BoundaryFence',     # Excessive missing data\n",
    "#     'ExtraFacility',     # Excessive missing data\n",
    "#     'ServiceLaneType',   # Excessive missing data\n",
    "# ]\n",
    "\n",
    "# X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "# X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# # --- 2. Target Transformation ---\n",
    "# # Log-transform the target variable\n",
    "# y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# # --- 3. Feature Engineering ---\n",
    "# def engineer_features(df):\n",
    "#     df = df.copy()\n",
    "    \n",
    "#     # 3.1 Age features\n",
    "#     df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "#     # Use max(ConstructionYear, RenovationYear) to get the most recent date\n",
    "#     df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "\n",
    "#     # 3.2 Interaction feature\n",
    "#     df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "#     # 3.3 Log transform selected skewed numerical features\n",
    "#     for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "#         if col in df.columns:\n",
    "#             # Impute with 0 for log transformation, then transform\n",
    "#             temp_df = df[col].fillna(0)\n",
    "#             df[col + '_Log'] = np.log1p(temp_df)\n",
    "\n",
    "#     # Drop original year-related columns to avoid multicollinearity\n",
    "#     df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# X_train_fe = engineer_features(X_train)\n",
    "# X_test_fe = engineer_features(X_test)\n",
    "\n",
    "\n",
    "# # --- 4. Feature Classification and Preprocessing Pipelines ---\n",
    "# numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "# categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# # Numerical Transformer: Impute missing with median\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median'))\n",
    "#     # No scaling is needed for Random Forest\n",
    "# ])\n",
    "\n",
    "# # Categorical Transformer: Impute missing with 'None' string, then One-Hot Encode\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, numerical_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "\n",
    "\n",
    "# # --- 5. Model Training (Random Forest Regressor) ---\n",
    "\n",
    "# # Using Random Forest for powerful non-linear modeling\n",
    "# rf_model_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     # n_jobs=-1 uses all available cores. max_depth controls complexity.\n",
    "#     ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=15))\n",
    "# ])\n",
    "\n",
    "# rf_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "\n",
    "\n",
    "# # --- 6. Prediction and Submission File Creation ---\n",
    "# y_test_log_pred = rf_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# # Reverse log-transformation to get predictions in the original dollar scale\n",
    "# y_test_pred = np.expm1(y_test_log_pred)\n",
    "# y_test_pred[y_test_pred < 0] = 0 # Ensure non-negative predictions\n",
    "\n",
    "# # Create and save the submission DataFrame\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'Id': test_ids,\n",
    "#     TARGET_COLUMN: y_test_pred\n",
    "# })\n",
    "\n",
    "# submission_file_name = 'random_forest_submission.csv'\n",
    "# submission_df.to_csv(submission_file_name, index=False)\n",
    "# print(f\"Submission file saved as '{submission_file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f20051c-5903-4c1f-b084-0803afe049d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Training data shape: (1200, 75)\n",
      "Test data shape: (260, 75)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 40\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1200, 79)\n",
      "\n",
      "--- 5. Model Training (LassoCV) ---\n",
      "Model training complete.\n",
      "LassoCV Optimal Alpha: 0.000638\n",
      "Training RMSE (Original Scale): 22,876.82\n",
      "Training R-squared: 0.9131\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'lasso_poly_submission.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151109.234540\n",
      "1  1106  327772.164773\n",
      "2   414  105405.649409\n",
      "3   523  155654.118290\n",
      "4  1037  320053.072460\n"
     ]
    }
   ],
   "source": [
    "#lasoo + polynomial regression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error # <-- CHANGE HERE\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    # Attempt to load from a common nested folder structure\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to the current directory\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        # Exit or raise error if data can't be loaded\n",
    "        raise\n",
    "\n",
    "X_train = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "# Columns to drop (based on the original code's specification)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType'\n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    # Time-based features\n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    # Interaction feature\n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    # Log transformation for skewed numerical features\n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    # Drop source columns used for feature engineering\n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute, Scale, and add Polynomial Features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "    # ('poly', PolynomialFeatures(degree=2, include_bias=False)) # <--- COMMENT THIS OUT\n",
    "])\n",
    "\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Lasso Regression with Cross-Validation) ---\n",
    "print(\"\\n--- 5. Model Training (LassoCV) ---\")\n",
    "lasso_model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LassoCV(cv=5, random_state=42, max_iter=10000)) \n",
    "])\n",
    "\n",
    "lasso_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Optional: Check model performance on training data\n",
    "y_train_log_pred = lasso_model_pipeline.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0 # Ensure no negative values after reverse transformation\n",
    "\n",
    "# Change the function call:\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred) # <-- FIX: Use root_mean_squared_error\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"LassoCV Optimal Alpha: {lasso_model_pipeline['regressor'].alpha_:.6f}\")\n",
    "print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = lasso_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 # Final check to ensure non-negative values\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'lasso_poly_submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dc51b2d-3391-4390-ae33-2145ad304fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "Cleaned training data shape: (1194, 73)\n",
      "Test data shape: (260, 73)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 38\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1194, 77)\n",
      "\n",
      "--- 5. Model Training (LassoCV) ---\n",
      "Model training complete.\n",
      "LassoCV Optimal Alpha: 0.000646\n",
      "Training RMSE (Original Scale): 17,477.56\n",
      "Training R-squared: 0.9453\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'lasso_submission_cleaned.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151947.354104\n",
      "1  1106  327944.333030\n",
      "2   414  104910.258353\n",
      "3   523  150776.861201\n",
      "4  1037  313606.712862\n"
     ]
    }
   ],
   "source": [
    "#lasso + cleaned data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    # Attempt to load from a common nested folder structure\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to the current directory\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        # Exit or raise error if data can't be loaded\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal (New Step) ---\n",
    "# Remove samples based on Target Value (extremely low/high values)\n",
    "# and based on large/extreme values in key predictor columns (UsableArea and OverallQuality).\n",
    "initial_row_count = len(df_train)\n",
    "\n",
    "# 1. Target-based cleaning: Remove extreme values (e.g., bottom 0.1% and top 0.1% of prices)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "# 2. Predictor-based cleaning (Common for this type of dataset)\n",
    "# Remove properties with extremely large UsableArea (e.g., > 4000 sq ft)\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "# Remove properties with poor OverallQuality and high UsableArea (often errors)\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "# Apply the mask to both features and target\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "\n",
    "# Sync test_ids for the remaining rows\n",
    "test_ids_cleaned = X_test['Id'] # No change to test IDs as we don't drop test rows\n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# Columns to drop (based on the original code's specification)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', 'BasementHalfBaths', 'LowQualityArea'\n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    # Time-based features\n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    # Interaction feature\n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    # Log transformation for skewed numerical features\n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    # Drop source columns used for feature engineering\n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute, Scale, and add Polynomial Features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "    # The degree=2 poly features were commented out in your previous code to speed things up. \n",
    "    # I'll keep them commented unless performance is a concern.\n",
    "    # ('poly', PolynomialFeatures(degree=2, include_bias=False)) \n",
    "])\n",
    "\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Lasso Regression with Cross-Validation) ---\n",
    "print(\"\\n--- 5. Model Training (LassoCV) ---\")\n",
    "lasso_model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LassoCV(cv=5, random_state=42, max_iter=10000)) \n",
    "])\n",
    "\n",
    "lasso_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Optional: Check model performance on training data\n",
    "y_train_log_pred = lasso_model_pipeline.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0 # Ensure no negative values after reverse transformation\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"LassoCV Optimal Alpha: {lasso_model_pipeline['regressor'].alpha_:.6f}\")\n",
    "print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = lasso_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 # Final check to ensure non-negative values\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'lasso_submission_cleaned.csv' # Changed filename to reflect cleaning\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b13db8c-74c0-4e01-b0ed-187bc82f821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "Cleaned training data shape: (1194, 73)\n",
      "Test data shape: (260, 73)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 38\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1194, 77)\n",
      "\n",
      "--- 5. Model Training (ElasticNetCV) ---\n",
      "Model training complete.\n",
      "ElasticNetCV Optimal Alpha: 0.000646\n",
      "ElasticNetCV Optimal L1 Ratio: 1.00\n",
      "Training RMSE (Original Scale): 17,477.56\n",
      "Training R-squared: 0.9453\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'elasticnet_submission_cleaned.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151947.354104\n",
      "1  1106  327944.333030\n",
      "2   414  104910.258353\n",
      "3   523  150776.861201\n",
      "4  1037  313606.712862\n"
     ]
    }
   ],
   "source": [
    "# # try elastic nets + maximum likelihood\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# # CHANGED: Import ElasticNetCV instead of LassoCV\n",
    "# from sklearn.linear_model import ElasticNetCV\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# # Define file paths\n",
    "# train_data_path = 'train.csv'\n",
    "# test_data_path = 'test.csv'\n",
    "# TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# # --- 1. Data Loading and Initial Setup ---\n",
    "# print(\"--- 1. Data Loading ---\")\n",
    "# try:\n",
    "#     # Attempt to load from a common nested folder structure\n",
    "#     df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "#     df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "#     print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "# except FileNotFoundError:\n",
    "#     # Fallback to the current directory\n",
    "#     try:\n",
    "#         df_train = pd.read_csv(train_data_path)\n",
    "#         df_test = pd.read_csv(test_data_path)\n",
    "#         print(\"Loaded data from current directory.\")\n",
    "#     except FileNotFoundError as e:\n",
    "#         print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "#         print(f\"Details: {e}\")\n",
    "#         # Exit or raise error if data can't be loaded\n",
    "#         raise\n",
    "\n",
    "# # Separate features and target before dropping/cleaning\n",
    "# X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "# y_train_raw = df_train[TARGET_COLUMN]\n",
    "# X_test = df_test.copy()\n",
    "# test_ids = X_test['Id']\n",
    "\n",
    "# print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# # --- 1.5 Outlier Removal ---\n",
    "# # Remove samples based on Target Value and key Predictor columns.\n",
    "# initial_row_count = len(df_train)\n",
    "\n",
    "# # 1. Target-based cleaning: Remove extreme values (e.g., bottom 0.1% and top 0.1% of prices)\n",
    "# y_lower_bound = y_train_raw.quantile(0.001)\n",
    "# y_upper_bound = y_train_raw.quantile(0.999)\n",
    "# outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "# # 2. Predictor-based cleaning (Common for this type of dataset)\n",
    "# # Remove properties with extremely large UsableArea (e.g., > 4000 sq ft)\n",
    "# if 'UsableArea' in X_train_raw.columns:\n",
    "#     outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "# # Remove properties with poor OverallQuality and high UsableArea (often errors)\n",
    "# if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "#     outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "# # Apply the mask to both features and target\n",
    "# X_train = X_train_raw[outlier_mask].copy()\n",
    "# y_train = y_train_raw[outlier_mask].copy()\n",
    "\n",
    "# # Note: test_ids_cleaned is used for the submission df but X_test is not filtered\n",
    "# test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "# print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# # Columns to drop (based on the original code's specification)\n",
    "# columns_to_drop = [\n",
    "#     'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', 'BasementHalfBaths', 'LowQualityArea'\n",
    "# ]\n",
    "# X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "# X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "# print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# # --- 2. Target Transformation ---\n",
    "# y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# # --- 3. Feature Engineering ---\n",
    "# def engineer_features(df):\n",
    "#     df = df.copy()\n",
    "#     # Time-based features\n",
    "#     df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "#     df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "#     # Interaction feature\n",
    "#     df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "#     # Log transformation for skewed numerical features\n",
    "#     for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "#         if col in df.columns:\n",
    "#             temp_df = df[col].fillna(0)\n",
    "#             df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "#     # Drop source columns used for feature engineering\n",
    "#     df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "#     return df\n",
    "\n",
    "# X_train_fe = engineer_features(X_train)\n",
    "# X_test_fe = engineer_features(X_test)\n",
    "\n",
    "# numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "# categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "# print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "# print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "# print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# # --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# # Numerical Transformer: Impute, Scale, and add Polynomial Features\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler()), \n",
    "#     # ('poly', PolynomialFeatures(degree=2, include_bias=False)) # Kept commented for speed\n",
    "# ])\n",
    "\n",
    "\n",
    "# # Categorical Transformer: Impute and One-Hot Encode\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, numerical_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "\n",
    "\n",
    "# # --- 5. Model Training (Elastic Net Regression with Cross-Validation) ---\n",
    "# print(\"\\n--- 5. Model Training (ElasticNetCV) ---\")\n",
    "# # CHANGED: Use ElasticNetCV. l1_ratio defines the mix between L1 (Lasso) and L2 (Ridge).\n",
    "# # We search over a range of ratios to find the best balance.\n",
    "# elastic_net_model_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', ElasticNetCV(\n",
    "#         l1_ratio=[.1, .5, .7, .9, .95, .99, 1], # Search for optimal L1/L2 mix\n",
    "#         cv=5, \n",
    "#         random_state=42, \n",
    "#         max_iter=10000\n",
    "#     )) \n",
    "# ])\n",
    "\n",
    "# elastic_net_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "# print(\"Model training complete.\")\n",
    "\n",
    "# # Optional: Check model performance on training data\n",
    "# y_train_log_pred = elastic_net_model_pipeline.predict(X_train_fe)\n",
    "# y_train_pred = np.expm1(y_train_log_pred)\n",
    "# y_train_pred[y_train_pred < 0] = 0 # Ensure no negative values after reverse transformation\n",
    "\n",
    "# rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "# r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# print(f\"ElasticNetCV Optimal Alpha: {elastic_net_model_pipeline['regressor'].alpha_:.6f}\")\n",
    "# print(f\"ElasticNetCV Optimal L1 Ratio: {elastic_net_model_pipeline['regressor'].l1_ratio_:.2f}\")\n",
    "# print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "# print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# # --- 6. Prediction and Submission File Creation ---\n",
    "# print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "# y_test_log_pred = elastic_net_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# # Reverse log-transformation\n",
    "# y_test_pred = np.expm1(y_test_log_pred)\n",
    "# y_test_pred[y_test_pred < 0] = 0 # Final check to ensure non-negative values\n",
    "\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'Id': test_ids_cleaned,\n",
    "#     TARGET_COLUMN: y_test_pred\n",
    "# })\n",
    "\n",
    "# submission_filename = 'elasticnet_submission_cleaned.csv' # Changed filename\n",
    "# submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "# print(\"Prediction process complete.\")\n",
    "# print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "# print(\"First 5 test predictions:\")\n",
    "# print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
