{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef65d78e-07c3-4919-b2cf-cfc5b7894d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading and Initial Setup ---\n",
      "Submission file saved as 'random_forest_submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# #try random forest\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# # Define file paths. Assuming files are in the current working directory.\n",
    "# TARGET_COLUMN = 'HotelValue'\n",
    "# train_data_path = 'train.csv'\n",
    "# test_data_path = 'test.csv'\n",
    "\n",
    "\n",
    "# # --- 1. Data Loading and Initial Setup ---\n",
    "# print(\"--- 1. Data Loading and Initial Setup ---\")\n",
    "# # Attempt to load files (using a robust method to handle potential path changes)\n",
    "# try:\n",
    "#     df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "#     df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "# except FileNotFoundError:\n",
    "#     df_train = pd.read_csv(train_data_path)\n",
    "#     df_test = pd.read_csv(test_data_path)\n",
    "\n",
    "# X_train = df_train.drop(columns=[TARGET_COLUMN])\n",
    "# y_train = df_train[TARGET_COLUMN]\n",
    "# X_test = df_test.copy()\n",
    "\n",
    "# test_ids = X_test['Id']\n",
    "\n",
    "# # Columns to drop due to irrelevance or excessive missing values\n",
    "# columns_to_drop = [\n",
    "#     'Id',\n",
    "#     'PoolQuality',       # Excessive missing data\n",
    "#     'BoundaryFence',     # Excessive missing data\n",
    "#     'ExtraFacility',     # Excessive missing data\n",
    "#     'ServiceLaneType',   # Excessive missing data\n",
    "# ]\n",
    "\n",
    "# X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "# X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# # --- 2. Target Transformation ---\n",
    "# # Log-transform the target variable\n",
    "# y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# # --- 3. Feature Engineering ---\n",
    "# def engineer_features(df):\n",
    "#     df = df.copy()\n",
    "    \n",
    "#     # 3.1 Age features\n",
    "#     df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "#     # Use max(ConstructionYear, RenovationYear) to get the most recent date\n",
    "#     df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "\n",
    "#     # 3.2 Interaction feature\n",
    "#     df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "#     # 3.3 Log transform selected skewed numerical features\n",
    "#     for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "#         if col in df.columns:\n",
    "#             # Impute with 0 for log transformation, then transform\n",
    "#             temp_df = df[col].fillna(0)\n",
    "#             df[col + '_Log'] = np.log1p(temp_df)\n",
    "\n",
    "#     # Drop original year-related columns to avoid multicollinearity\n",
    "#     df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# X_train_fe = engineer_features(X_train)\n",
    "# X_test_fe = engineer_features(X_test)\n",
    "\n",
    "\n",
    "# # --- 4. Feature Classification and Preprocessing Pipelines ---\n",
    "# numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "# categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# # Numerical Transformer: Impute missing with median\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median'))\n",
    "#     # No scaling is needed for Random Forest\n",
    "# ])\n",
    "\n",
    "# # Categorical Transformer: Impute missing with 'None' string, then One-Hot Encode\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, numerical_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "\n",
    "\n",
    "# # --- 5. Model Training (Random Forest Regressor) ---\n",
    "\n",
    "# # Using Random Forest for powerful non-linear modeling\n",
    "# rf_model_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     # n_jobs=-1 uses all available cores. max_depth controls complexity.\n",
    "#     ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=15))\n",
    "# ])\n",
    "\n",
    "# rf_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "\n",
    "\n",
    "# # --- 6. Prediction and Submission File Creation ---\n",
    "# y_test_log_pred = rf_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# # Reverse log-transformation to get predictions in the original dollar scale\n",
    "# y_test_pred = np.expm1(y_test_log_pred)\n",
    "# y_test_pred[y_test_pred < 0] = 0 # Ensure non-negative predictions\n",
    "\n",
    "# # Create and save the submission DataFrame\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'Id': test_ids,\n",
    "#     TARGET_COLUMN: y_test_pred\n",
    "# })\n",
    "\n",
    "# submission_file_name = 'random_forest_submission.csv'\n",
    "# submission_df.to_csv(submission_file_name, index=False)\n",
    "# print(f\"Submission file saved as '{submission_file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f20051c-5903-4c1f-b084-0803afe049d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Training data shape: (1200, 75)\n",
      "Test data shape: (260, 75)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 40\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1200, 79)\n",
      "\n",
      "--- 5. Model Training (LassoCV) ---\n",
      "Model training complete.\n",
      "LassoCV Optimal Alpha: 0.000638\n",
      "Training RMSE (Original Scale): 22,876.82\n",
      "Training R-squared: 0.9131\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'lasso_poly_submission.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151109.234540\n",
      "1  1106  327772.164773\n",
      "2   414  105405.649409\n",
      "3   523  155654.118290\n",
      "4  1037  320053.072460\n"
     ]
    }
   ],
   "source": [
    "#lasoo + polynomial regression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error # <-- CHANGE HERE\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    # Attempt to load from a common nested folder structure\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to the current directory\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        # Exit or raise error if data can't be loaded\n",
    "        raise\n",
    "\n",
    "X_train = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "# Columns to drop (based on the original code's specification)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType'\n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    # Time-based features\n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    # Interaction feature\n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    # Log transformation for skewed numerical features\n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    # Drop source columns used for feature engineering\n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute, Scale, and add Polynomial Features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "    # ('poly', PolynomialFeatures(degree=2, include_bias=False)) # <--- COMMENT THIS OUT\n",
    "])\n",
    "\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Lasso Regression with Cross-Validation) ---\n",
    "print(\"\\n--- 5. Model Training (LassoCV) ---\")\n",
    "lasso_model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LassoCV(cv=5, random_state=42, max_iter=10000)) \n",
    "])\n",
    "\n",
    "lasso_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Optional: Check model performance on training data\n",
    "y_train_log_pred = lasso_model_pipeline.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0 # Ensure no negative values after reverse transformation\n",
    "\n",
    "# Change the function call:\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred) # <-- FIX: Use root_mean_squared_error\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"LassoCV Optimal Alpha: {lasso_model_pipeline['regressor'].alpha_:.6f}\")\n",
    "print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = lasso_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 # Final check to ensure non-negative values\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'lasso_poly_submission.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc51b2d-3391-4390-ae33-2145ad304fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "Cleaned training data shape: (1194, 73)\n",
      "Test data shape: (260, 73)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 38\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1194, 77)\n",
      "\n",
      "--- 5. Model Training (LassoCV) ---\n",
      "Model training complete.\n",
      "LassoCV Optimal Alpha: 0.000646\n",
      "Training RMSE (Original Scale): 17,477.56\n",
      "Training R-squared: 0.9453\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'lasso_submission_cleaned.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151947.354104\n",
      "1  1106  327944.333030\n",
      "2   414  104910.258353\n",
      "3   523  150776.861201\n",
      "4  1037  313606.712862\n"
     ]
    }
   ],
   "source": [
    "#lasso + cleaned data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    # Attempt to load from a common nested folder structure\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to the current directory\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        # Exit or raise error if data can't be loaded\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal (New Step) ---\n",
    "# Remove samples based on Target Value (extremely low/high values)\n",
    "# and based on large/extreme values in key predictor columns (UsableArea and OverallQuality).\n",
    "initial_row_count = len(df_train)\n",
    "\n",
    "# 1. Target-based cleaning: Remove extreme values (e.g., bottom 0.1% and top 0.1% of prices)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "# 2. Predictor-based cleaning (Common for this type of dataset)\n",
    "# Remove properties with extremely large UsableArea (e.g., > 4000 sq ft)\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "# Remove properties with poor OverallQuality and high UsableArea (often errors)\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "# Apply the mask to both features and target\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "\n",
    "# Sync test_ids for the remaining rows\n",
    "test_ids_cleaned = X_test['Id'] # No change to test IDs as we don't drop test rows\n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# Columns to drop (based on the original code's specification)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', 'BasementHalfBaths', 'LowQualityArea'\n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    # Time-based features\n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    # Interaction feature\n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    # Log transformation for skewed numerical features\n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    # Drop source columns used for feature engineering\n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute, Scale, and add Polynomial Features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "    # The degree=2 poly features were commented out in your previous code to speed things up. \n",
    "    # I'll keep them commented unless performance is a concern.\n",
    "    # ('poly', PolynomialFeatures(degree=2, include_bias=False)) \n",
    "])\n",
    "\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Lasso Regression with Cross-Validation) ---\n",
    "print(\"\\n--- 5. Model Training (LassoCV) ---\")\n",
    "lasso_model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LassoCV(cv=5, random_state=42, max_iter=10000)) \n",
    "])\n",
    "\n",
    "lasso_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Optional: Check model performance on training data\n",
    "y_train_log_pred = lasso_model_pipeline.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0 # Ensure no negative values after reverse transformation\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"LassoCV Optimal Alpha: {lasso_model_pipeline['regressor'].alpha_:.6f}\")\n",
    "print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = lasso_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 # Final check to ensure non-negative values\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'lasso_submission_cleaned.csv' # Changed filename to reflect cleaning\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b13db8c-74c0-4e01-b0ed-187bc82f821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "Cleaned training data shape: (1194, 73)\n",
      "Test data shape: (260, 73)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 38\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1194, 77)\n",
      "\n",
      "--- 5. Model Training (ElasticNetCV) ---\n",
      "Model training complete.\n",
      "ElasticNetCV Optimal Alpha: 0.000646\n",
      "ElasticNetCV Optimal L1 Ratio: 1.00\n",
      "Training RMSE (Original Scale): 17,477.56\n",
      "Training R-squared: 0.9453\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'elasticnet_submission_cleaned.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151947.354104\n",
      "1  1106  327944.333030\n",
      "2   414  104910.258353\n",
      "3   523  150776.861201\n",
      "4  1037  313606.712862\n"
     ]
    }
   ],
   "source": [
    "# # try elastic nets + maximum likelihood\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# # CHANGED: Import ElasticNetCV instead of LassoCV\n",
    "# from sklearn.linear_model import ElasticNetCV\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# # Define file paths\n",
    "# train_data_path = 'train.csv'\n",
    "# test_data_path = 'test.csv'\n",
    "# TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# # --- 1. Data Loading and Initial Setup ---\n",
    "# print(\"--- 1. Data Loading ---\")\n",
    "# try:\n",
    "#     # Attempt to load from a common nested folder structure\n",
    "#     df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "#     df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "#     print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "# except FileNotFoundError:\n",
    "#     # Fallback to the current directory\n",
    "#     try:\n",
    "#         df_train = pd.read_csv(train_data_path)\n",
    "#         df_test = pd.read_csv(test_data_path)\n",
    "#         print(\"Loaded data from current directory.\")\n",
    "#     except FileNotFoundError as e:\n",
    "#         print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "#         print(f\"Details: {e}\")\n",
    "#         # Exit or raise error if data can't be loaded\n",
    "#         raise\n",
    "\n",
    "# # Separate features and target before dropping/cleaning\n",
    "# X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "# y_train_raw = df_train[TARGET_COLUMN]\n",
    "# X_test = df_test.copy()\n",
    "# test_ids = X_test['Id']\n",
    "\n",
    "# print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# # --- 1.5 Outlier Removal ---\n",
    "# # Remove samples based on Target Value and key Predictor columns.\n",
    "# initial_row_count = len(df_train)\n",
    "\n",
    "# # 1. Target-based cleaning: Remove extreme values (e.g., bottom 0.1% and top 0.1% of prices)\n",
    "# y_lower_bound = y_train_raw.quantile(0.001)\n",
    "# y_upper_bound = y_train_raw.quantile(0.999)\n",
    "# outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "# # 2. Predictor-based cleaning (Common for this type of dataset)\n",
    "# # Remove properties with extremely large UsableArea (e.g., > 4000 sq ft)\n",
    "# if 'UsableArea' in X_train_raw.columns:\n",
    "#     outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "# # Remove properties with poor OverallQuality and high UsableArea (often errors)\n",
    "# if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "#     outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "# # Apply the mask to both features and target\n",
    "# X_train = X_train_raw[outlier_mask].copy()\n",
    "# y_train = y_train_raw[outlier_mask].copy()\n",
    "\n",
    "# # Note: test_ids_cleaned is used for the submission df but X_test is not filtered\n",
    "# test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "# print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# # Columns to drop (based on the original code's specification)\n",
    "# columns_to_drop = [\n",
    "#     'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', 'BasementHalfBaths', 'LowQualityArea'\n",
    "# ]\n",
    "# X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "# X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "# print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# # --- 2. Target Transformation ---\n",
    "# y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# # --- 3. Feature Engineering ---\n",
    "# def engineer_features(df):\n",
    "#     df = df.copy()\n",
    "#     # Time-based features\n",
    "#     df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "#     df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "#     # Interaction feature\n",
    "#     df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "#     # Log transformation for skewed numerical features\n",
    "#     for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "#         if col in df.columns:\n",
    "#             temp_df = df[col].fillna(0)\n",
    "#             df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "#     # Drop source columns used for feature engineering\n",
    "#     df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "#     return df\n",
    "\n",
    "# X_train_fe = engineer_features(X_train)\n",
    "# X_test_fe = engineer_features(X_test)\n",
    "\n",
    "# numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "# categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "# print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "# print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "# print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# # --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# # Numerical Transformer: Impute, Scale, and add Polynomial Features\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler()), \n",
    "#     # ('poly', PolynomialFeatures(degree=2, include_bias=False)) # Kept commented for speed\n",
    "# ])\n",
    "\n",
    "\n",
    "# # Categorical Transformer: Impute and One-Hot Encode\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, numerical_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "\n",
    "\n",
    "# # --- 5. Model Training (Elastic Net Regression with Cross-Validation) ---\n",
    "# print(\"\\n--- 5. Model Training (ElasticNetCV) ---\")\n",
    "# # CHANGED: Use ElasticNetCV. l1_ratio defines the mix between L1 (Lasso) and L2 (Ridge).\n",
    "# # We search over a range of ratios to find the best balance.\n",
    "# elastic_net_model_pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', ElasticNetCV(\n",
    "#         l1_ratio=[.1, .5, .7, .9, .95, .99, 1], # Search for optimal L1/L2 mix\n",
    "#         cv=5, \n",
    "#         random_state=42, \n",
    "#         max_iter=10000\n",
    "#     )) \n",
    "# ])\n",
    "\n",
    "# elastic_net_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "# print(\"Model training complete.\")\n",
    "\n",
    "# # Optional: Check model performance on training data\n",
    "# y_train_log_pred = elastic_net_model_pipeline.predict(X_train_fe)\n",
    "# y_train_pred = np.expm1(y_train_log_pred)\n",
    "# y_train_pred[y_train_pred < 0] = 0 # Ensure no negative values after reverse transformation\n",
    "\n",
    "# rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "# r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# print(f\"ElasticNetCV Optimal Alpha: {elastic_net_model_pipeline['regressor'].alpha_:.6f}\")\n",
    "# print(f\"ElasticNetCV Optimal L1 Ratio: {elastic_net_model_pipeline['regressor'].l1_ratio_:.2f}\")\n",
    "# print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "# print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# # --- 6. Prediction and Submission File Creation ---\n",
    "# print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "# y_test_log_pred = elastic_net_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# # Reverse log-transformation\n",
    "# y_test_pred = np.expm1(y_test_log_pred)\n",
    "# y_test_pred[y_test_pred < 0] = 0 # Final check to ensure non-negative values\n",
    "\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'Id': test_ids_cleaned,\n",
    "#     TARGET_COLUMN: y_test_pred\n",
    "# })\n",
    "\n",
    "# submission_filename = 'elasticnet_submission_cleaned.csv' # Changed filename\n",
    "# submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "# print(\"Prediction process complete.\")\n",
    "# print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "# print(\"First 5 test predictions:\")\n",
    "# print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aabaa28-896c-48ae-a1e1-b46ce032737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "Cleaned training data shape: (1194, 75)\n",
      "Test data shape: (260, 75)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 40\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1194, 79)\n",
      "\n",
      "--- 5. Model Training (BayesianRidge) ---\n",
      "Model training complete.\n",
      "BayesianRidge Estimated Alpha (Noise Precision): 107.463893\n",
      "BayesianRidge Estimated Lambda (Weight Precision): 1656.384294\n",
      "Training RMSE (Original Scale): 16,770.82\n",
      "Training R-squared: 0.9497\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'bayesianridge_submission_cleaned.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  152026.789240\n",
      "1  1106  328480.329555\n",
      "2   414  105060.132452\n",
      "3   523  154274.255646\n",
      "4  1037  315312.441403\n"
     ]
    }
   ],
   "source": [
    "#try bayesian lasoo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Using BayesianRidge for a Bayesian approach to linear regression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    # Attempt to load from a common nested folder structure\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to the current directory\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "# Predictor-based cleaning (UsableArea and OverallQuality)\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "# Apply the mask to both features and target\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType'\n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "    # ('poly', PolynomialFeatures(degree=2, include_bias=False)) \n",
    "])\n",
    "\n",
    "# Categorical Transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    # CRITICAL FIX: Changed 'sparse_output=False' to 'sparse_threshold=0' \n",
    "    # for modern Scikit-learn versions (>= 1.2).\n",
    "    sparse_threshold=0.0\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Bayesian Ridge Regression) ---\n",
    "print(\"\\n--- 5. Model Training (BayesianRidge) ---\")\n",
    "bayesian_model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # FIX: Using 'max_iter' for modern scikit-learn compatibility\n",
    "    ('regressor', BayesianRidge(max_iter=10000)) \n",
    "])\n",
    "\n",
    "bayesian_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Optional: Check model performance on training data\n",
    "y_train_log_pred = bayesian_model_pipeline.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0 \n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "alpha_est = bayesian_model_pipeline['regressor'].alpha_\n",
    "lambda_est = bayesian_model_pipeline['regressor'].lambda_\n",
    "\n",
    "print(f\"BayesianRidge Estimated Alpha (Noise Precision): {alpha_est:.6f}\")\n",
    "print(f\"BayesianRidge Estimated Lambda (Weight Precision): {lambda_est:.6f}\")\n",
    "print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = bayesian_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'bayesianridge_submission_cleaned.csv' \n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4e7b15-4ce6-4b8c-b7c9-c236ac852628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "Cleaned training data shape: (1194, 75)\n",
      "Test data shape: (260, 75)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 40\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1194, 79)\n",
      "\n",
      "--- 5. Model Training (BayesianRidge) ---\n",
      "Model training complete.\n",
      "BayesianRidge Estimated Alpha (Noise Precision): 107.463893\n",
      "BayesianRidge Estimated Lambda (Weight Precision): 1656.384294\n",
      "Training RMSE (Original Scale): 16,770.82\n",
      "Training R-squared: 0.9497\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'bayesianridge_submission_cleaned.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  152026.789240\n",
      "1  1106  328480.329555\n",
      "2   414  105060.132452\n",
      "3   523  154274.255646\n",
      "4  1037  315312.441403\n"
     ]
    }
   ],
   "source": [
    "#bayesian ridge\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Using BayesianRidge (closest native Scikit-learn Bayesian model)\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType'\n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "# Categorical Transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    # FIX: Use 'sparse_threshold=0.0' for modern Scikit-learn dense output\n",
    "    sparse_threshold=0.0\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Bayesian Ridge Regression - Closest Scikit-learn Bayesian Model) ---\n",
    "print(\"\\n--- 5. Model Training (BayesianRidge) ---\")\n",
    "# NOTE: Scikit-learn does not have a native 'BayesianLasso' model. \n",
    "# We use BayesianRidge, which assumes Gaussian priors (Ridge behavior).\n",
    "# For true Bayesian Lasso (Laplace priors), a library like PyMC or Stan would be required.\n",
    "bayesian_model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', BayesianRidge(max_iter=10000)) \n",
    "])\n",
    "\n",
    "bayesian_model_pipeline.fit(X_train_fe, y_train_log)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Optional: Check model performance on training data\n",
    "y_train_log_pred = bayesian_model_pipeline.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0 \n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "alpha_est = bayesian_model_pipeline['regressor'].alpha_\n",
    "lambda_est = bayesian_model_pipeline['regressor'].lambda_\n",
    "\n",
    "print(f\"BayesianRidge Estimated Alpha (Noise Precision): {alpha_est:.6f}\")\n",
    "print(f\"BayesianRidge Estimated Lambda (Weight Precision): {lambda_est:.6f}\")\n",
    "print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = bayesian_model_pipeline.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'bayesianridge_submission_cleaned.csv' \n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cad3afe-da4c-4fa3-94f1-0866e8a90eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "Cleaned training data shape: (1194, 75)\n",
      "Test data shape: (260, 75)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 40\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1194, 79)\n",
      "\n",
      "--- 5. Model Training (XGBRegressor) ---\n",
      "Model training complete.\n",
      "XGBRegressor Best Iteration: 252\n",
      "Training RMSE (Original Scale): 11,267.07\n",
      "Training R-squared: 0.9773\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'xgboost_submission_final.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  144160.406250\n",
      "1  1106  307986.218750\n",
      "2   414  110486.804688\n",
      "3   523  145697.625000\n",
      "4  1037  329098.656250\n"
     ]
    }
   ],
   "source": [
    "#xgboost try1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split \n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType'\n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines (Simplified for Tree-Based Model) ---\n",
    "\n",
    "# Numerical Transformer: Only Impute\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "# Categorical Transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0.0\n",
    ")\n",
    "\n",
    "# --- 4.5 Manual Preprocessing & Feature Alignment (CRITICAL FIX) ---\n",
    "\n",
    "# 1. Fit the preprocessor on the ENTIRE training set (X_train_fe) to learn all categories/imputation values.\n",
    "#    This ensures a consistent feature set size for all subsequent data splits.\n",
    "X_train_processed = preprocessor.fit_transform(X_train_fe)\n",
    "X_test_processed = preprocessor.transform(X_test_fe)\n",
    "\n",
    "# 2. Split the *preprocessed* training data (now an array) for fitting and validation.\n",
    "X_fit_processed, X_val_processed, y_fit, y_val = train_test_split(\n",
    "    X_train_processed, y_train_log, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create a simplified pipeline that just contains the regressor (since preprocessing is done).\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror', \n",
    "    n_estimators=1000, \n",
    "    learning_rate=0.05, \n",
    "    max_depth=4, \n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (XGBRegressor - Boosting Method) ---\n",
    "print(\"\\n--- 5. Model Training (XGBRegressor) ---\")\n",
    "\n",
    "# The eval_set must contain tuples of (features, target)\n",
    "eval_set_processed = [(X_val_processed, y_val)]\n",
    "\n",
    "# Fit the XGBoost model directly (outside of a pipeline) on the numerical arrays.\n",
    "xgb_model.fit(\n",
    "    X_fit_processed, y_fit, \n",
    "    eval_set=eval_set_processed, \n",
    "    verbose=False\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Optional: Check model performance on training data\n",
    "# Note: Predict on the full preprocessed training set (X_train_processed)\n",
    "y_train_log_pred = xgb_model.predict(X_train_processed)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0 \n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"XGBRegressor Best Iteration: {xgb_model.best_iteration}\")\n",
    "print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "# Predict on the preprocessed test set (X_test_processed)\n",
    "y_test_log_pred = xgb_model.predict(X_test_processed)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'xgboost_submission_final.csv' \n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc367176-e9f5-4150-85da-0e401a622a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "Cleaned training data shape: (1194, 75)\n",
      "Test data shape: (260, 75)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 40\n",
      "Number of categorical features: 39\n",
      "Final training features shape: (1194, 79)\n",
      "\n",
      "--- 5. Model Training (XGBRegressor) ---\n",
      "Model training complete.\n",
      "XGBRegressor Best Iteration: 580\n",
      "Training RMSE (Original Scale): 18,511.73\n",
      "Training R-squared: 0.9387\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'xgboost_submission_regularized.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  140108.921875\n",
      "1  1106  325542.875000\n",
      "2   414  117880.695312\n",
      "3   523  149607.937500\n",
      "4  1037  310675.750000\n"
     ]
    }
   ],
   "source": [
    "#xgboost try2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split \n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        raise\n",
    "\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType'\n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Only Impute\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "# Categorical Transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0.0\n",
    ")\n",
    "\n",
    "# --- 4.5 Manual Preprocessing & Feature Alignment ---\n",
    "\n",
    "# Fit the preprocessor on the ENTIRE training set (X_train_fe)\n",
    "X_train_processed = preprocessor.fit_transform(X_train_fe)\n",
    "X_test_processed = preprocessor.transform(X_test_fe)\n",
    "\n",
    "# Split the *preprocessed* training data (now an array) for fitting and validation.\n",
    "X_fit_processed, X_val_processed, y_fit, y_val = train_test_split(\n",
    "    X_train_processed, y_train_log, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create the XGBoost model with regularization parameters (to prevent overfitting).\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror', \n",
    "    n_estimators=2000, \n",
    "    learning_rate=0.01,         # Reduced learning rate (smaller steps)\n",
    "    max_depth=3,                # Reduced max_depth (simpler trees)\n",
    "    min_child_weight=5,         # Increased min_child_weight (more regularization)\n",
    "    gamma=0.1,                  # Added minimum loss reduction for splits\n",
    "    reg_alpha=0.1,              # Added L1 regularization (Lasso)\n",
    "    reg_lambda=1.0,             # Added L2 regularization (Ridge)\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=100,  # Increased early stopping rounds\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (XGBRegressor - Boosting Method) ---\n",
    "print(\"\\n--- 5. Model Training (XGBRegressor) ---\")\n",
    "\n",
    "eval_set_processed = [(X_val_processed, y_val)]\n",
    "\n",
    "# Fit the XGBoost model directly on the numerical arrays.\n",
    "xgb_model.fit(\n",
    "    X_fit_processed, y_fit, \n",
    "    eval_set=eval_set_processed, \n",
    "    verbose=False # Set to True to see early stopping results\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Optional: Check model performance on training data\n",
    "y_train_log_pred = xgb_model.predict(X_train_processed)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0 \n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"XGBRegressor Best Iteration: {xgb_model.best_iteration}\")\n",
    "print(f\"Training RMSE (Original Scale): {rmse_train:,.2f}\")\n",
    "print(f\"Training R-squared: {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = xgb_model.predict(X_test_processed)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'xgboost_submission_regularized.csv' \n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5663e682-88cd-4493-b3ed-8ef9691bc9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 66)\n",
      "Test data shape: (260, 66)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 34\n",
      "Number of categorical features: 37\n",
      "Final training features shape: (1194, 71)\n",
      "\n",
      "--- 5. Model Training (LassoCV with Polynomial Features) ---\n",
      "Model training complete.\n",
      "LassoCV Optimal Alpha: 0.001725\n",
      "\n",
      "Polynomial Lasso RMSE (Train): 16,886.31\n",
      "Polynomial Lasso R² (Train): 0.9490\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'lasso_poly_submission_final.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  148924.126945\n",
      "1  1106  335121.773030\n",
      "2   414  109544.419398\n",
      "3   523  157249.584302\n",
      "4  1037  315144.759281\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# CHANGED: Using LassoCV for regularization and feature selection\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold # Kept for potential future use\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    # Attempt to load from a common nested folder structure\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to the current directory\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal (Kept) ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging (Kept) ---\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "             errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (including multicollinearity removal)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    # Multicollinearity removal\n",
    "    'ParkingCapacity', \n",
    "    'GroundFloorArea', \n",
    "    'TotalRooms', \n",
    "    'UpperFloorArea', \n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering (Kept) ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines (Modified) ---\n",
    "\n",
    "# Numerical Transformer: Impute, Scale, and ADD Polynomial Features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "    # CRITICAL CHANGE: Add PolynomialFeatures to generate non-linear response\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)) \n",
    "])\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (LassoCV - Regularized Non-linear Fit) ---\n",
    "print(\"\\n--- 5. Model Training (LassoCV with Polynomial Features) ---\")\n",
    "\n",
    "# LassoCV automatically tunes the regularization strength (alpha) using cross-validation.\n",
    "lasso_poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LassoCV(cv=5, random_state=42, max_iter=10000, n_jobs=-1)) \n",
    "])\n",
    "\n",
    "lasso_poly_pipeline.fit(X_train_fe, y_train_log)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate optimized model\n",
    "best_model = lasso_poly_pipeline\n",
    "\n",
    "y_train_log_pred = best_model.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"LassoCV Optimal Alpha: {best_model['regressor'].alpha_:.6f}\")\n",
    "print(f\"\\nPolynomial Lasso RMSE (Train): {rmse_train:,.2f}\")\n",
    "print(f\"Polynomial Lasso R² (Train): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = best_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'lasso_poly_submission_final.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a1b845-6353-4683-8782-94dd0fa07b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 66)\n",
      "Test data shape: (260, 66)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 34\n",
      "Number of categorical features: 37\n",
      "Final training features shape: (1194, 71)\n",
      "\n",
      "--- 5A. Baseline Linear Regression ---\n",
      "Linear Regression RMSE (Train): 15,318.01\n",
      "Linear Regression R² (Train): 0.9580\n",
      "\n",
      "--- 5B. Bayesian Optimization for Linear Regression ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best Bayesian Hyperparameters:\n",
      "OrderedDict({'regressor__alpha': 10.0, 'regressor__fit_intercept': True, 'regressor__tol': 0.0001685299475486865})\n",
      "\n",
      "Bayesian Optimized Linear Regression RMSE (Train): 16,685.94\n",
      "Bayesian Optimized Linear Regression R² (Train): 0.9502\n",
      "\n",
      "--- Model Comparison ---\n",
      "Baseline Linear Regression RMSE: 15,318.01 | R²: 0.9580\n",
      "Bayesian Optimized Regression RMSE: 16,685.94 | R²: 0.9502\n",
      "✅ Using baseline Linear Regression as final model (performed better or equal).\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Prediction process complete.\n",
      "Submission file 'linear_submission_cleaned.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  153117.083933\n",
      "1  1106  326460.701557\n",
      "2   414  101378.110401\n",
      "3   523  160397.581578\n",
      "4  1037  320780.014080\n"
     ]
    }
   ],
   "source": [
    "#sid wala best\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    # Attempt to load from a common nested folder structure\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to the current directory\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        # Exit or raise error if data can't be loaded\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal (New Step) ---\n",
    "# Remove samples based on Target Value (extremely low/high values)\n",
    "# and based on large/extreme values in key predictor columns (UsableArea and OverallQuality).\n",
    "initial_row_count = len(df_train)\n",
    "\n",
    "# 1. Target-based cleaning: Remove extreme values (e.g., bottom 0.1% and top 0.1% of prices)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "# 2. Predictor-based cleaning (Common for this type of dataset)\n",
    "# Remove properties with extremely large UsableArea (e.g., > 4000 sq ft)\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "# Remove properties with poor OverallQuality and high UsableArea (often errors)\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "# Apply the mask to both features and target\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "\n",
    "# Sync test_ids for the remaining rows\n",
    "test_ids_cleaned = X_test['Id'] # No change to test IDs as we don't drop test rows\n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging (Before Dropping Columns) ---\n",
    "# Merge Basement Features into Weighted Quality Score\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    # Fill NaN values\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    # Map types to scores\n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    # Calculate weighted quality score\n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    # Average basement quality (cast to float to avoid dtype warning)\n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    # Drop original basement facility columns\n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "# Merge Porch/Veranda Features\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "            errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (including multicollinearity removal)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    # Multicollinearity removal\n",
    "    'ParkingCapacity',  # Keep ParkingArea\n",
    "    'GroundFloorArea',  # Keep UsableArea\n",
    "    'TotalRooms',       # Keep FullBaths\n",
    "    'UpperFloorArea',   # Captured in UsableArea\n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    # Handle RenovationYear: if 0 or missing, use ConstructionYear\n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    # Interaction features\n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    # Bathroom quality feature\n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    # Log transformation for skewed numerical features\n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    # Drop source columns used for feature engineering\n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute, Scale, and add Polynomial Features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "    # The degree=2 poly features were commented out in your previous code to speed things up. \n",
    "    # I'll keep them commented unless performance is a concern.\n",
    "    # ('poly', PolynomialFeatures(degree=2, include_bias=False)) \n",
    "])\n",
    "\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 5. Model Training: Linear Regression + Bayesian Optimization ---\n",
    "print(\"\\n--- 5A. Baseline Linear Regression ---\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Baseline Linear Regression pipeline\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train baseline model\n",
    "lr_pipeline.fit(X_train_fe, y_train_log)\n",
    "\n",
    "# Evaluate baseline model\n",
    "y_train_log_pred = lr_pipeline.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train_lr = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train_lr = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"Linear Regression RMSE (Train): {rmse_train_lr:,.2f}\")\n",
    "print(f\"Linear Regression R² (Train): {r2_train_lr:.4f}\")\n",
    "\n",
    "\n",
    "# --- 5B. Bayesian Optimization for Linear Regression ---\n",
    "print(\"\\n--- 5B. Bayesian Optimization for Linear Regression ---\")\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Real\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# We'll wrap Linear Regression in Ridge to allow tuning small alpha (acts as regularization)\n",
    "# Because pure LinearRegression has almost no hyperparameters to tune\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define Bayesian search space\n",
    "search_space = {\n",
    "    'regressor__alpha': Real(1e-6, 1e1, prior='log-uniform'),  # small regularization\n",
    "    'regressor__fit_intercept': Categorical([True, False]),\n",
    "    'regressor__tol': Real(1e-5, 1e-2, prior='log-uniform')\n",
    "}\n",
    "\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=ridge_pipeline,\n",
    "    search_spaces=search_space,\n",
    "    n_iter=30,\n",
    "    cv=cv_strategy,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_fe, y_train_log)\n",
    "\n",
    "print(\"\\nBest Bayesian Hyperparameters:\")\n",
    "print(bayes_search.best_params_)\n",
    "\n",
    "# Evaluate optimized model\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "y_train_log_pred_bayes = best_model.predict(X_train_fe)\n",
    "y_train_pred_bayes = np.expm1(y_train_log_pred_bayes)\n",
    "y_train_pred_bayes[y_train_pred_bayes < 0] = 0\n",
    "\n",
    "rmse_train_bayes = root_mean_squared_error(y_train, y_train_pred_bayes)\n",
    "r2_train_bayes = r2_score(y_train, y_train_pred_bayes)\n",
    "\n",
    "print(f\"\\nBayesian Optimized Linear Regression RMSE (Train): {rmse_train_bayes:,.2f}\")\n",
    "print(f\"Bayesian Optimized Linear Regression R² (Train): {r2_train_bayes:.4f}\")\n",
    "\n",
    "\n",
    "# --- Compare and Select Best Model ---\n",
    "print(\"\\n--- Model Comparison ---\")\n",
    "print(f\"Baseline Linear Regression RMSE: {rmse_train_lr:,.2f} | R²: {r2_train_lr:.4f}\")\n",
    "print(f\"Bayesian Optimized Regression RMSE: {rmse_train_bayes:,.2f} | R²: {r2_train_bayes:.4f}\")\n",
    "\n",
    "if rmse_train_bayes < rmse_train_lr:\n",
    "    final_model = best_model\n",
    "    print(\"✅ Using Bayesian-optimized Linear Regression as final model.\")\n",
    "else:\n",
    "    final_model = lr_pipeline\n",
    "    print(\"✅ Using baseline Linear Regression as final model (performed better or equal).\")\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = final_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 # Final check to ensure non-negative values\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'linear_submission_cleaned.csv' # Changed filename to reflect cleaning\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"Prediction process complete.\")\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900ebaa-29fb-4052-b7da-800c1dd3a523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 66)\n",
      "Test data shape: (260, 66)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 34\n",
      "Number of categorical features: 37\n",
      "Final training features shape: (1194, 71)\n",
      "\n",
      "--- 5. Model Training (Polynomial Regression + Bayesian Optimization) ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "#polynomial bayesian\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge # Changed from LassoCV to Ridge for Bayes search\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Real, Integer\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    # Attempt to load from a common nested folder structure\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback to the current directory\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging ---\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['OpenVerandaArea'].fillna(0)+\n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea', 'OpenVarandaArea'], \n",
    "             errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (including multicollinearity removal)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    'ParkingCapacity', \n",
    "    'GroundFloorArea', \n",
    "    'TotalRooms', \n",
    "    'UpperFloorArea', \n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute and Scale (Polynomials will be added in the final pipeline)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Polynomial Regression + Bayesian Optimization) ---\n",
    "print(\"\\n--- 5. Model Training (Polynomial Regression + Bayesian Optimization) ---\")\n",
    "\n",
    "# Define the pipeline: Preprocessor + PolynomialFeatures + Ridge Regression\n",
    "poly_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('regressor', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "# Define search space for Bayesian Optimization\n",
    "search_space = {\n",
    "    'poly__degree': Integer(1, 3),      # Non-linearity via degree\n",
    "    'regressor__alpha': Real(1e-3, 100, prior='log-uniform') # Regularization\n",
    "}\n",
    "\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=poly_ridge_pipeline,\n",
    "    search_spaces=search_space,\n",
    "    n_iter=20, # number of optimization iterations\n",
    "    cv=cv_strategy,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_fe, y_train_log)\n",
    "\n",
    "print(\"\\nBest Bayesian Hyperparameters:\")\n",
    "print(bayes_search.best_params_)\n",
    "\n",
    "# Train final model using best parameters\n",
    "final_model = bayes_search.best_estimator_\n",
    "\n",
    "# Evaluate optimized model on training data\n",
    "y_train_log_pred = final_model.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nPolynomial Ridge RMSE (Train): {rmse_train:,.2f}\")\n",
    "print(f\"Polynomial Ridge R² (Train): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = final_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 # Final check to ensure non-negative values\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'poly_bayesian_submission_final.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34132903-35eb-4485-97cd-1443be504043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merged quality/condition features into TotalQualityScore.\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 59)\n",
      "Test data shape: (260, 59)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 35\n",
      "Number of categorical features: 29\n",
      "Final training features shape: (1194, 64)\n",
      "\n",
      "--- 5. Model Training (Bayesian Optimized Ridge) ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best Bayesian Hyperparameters:\n",
      "OrderedDict({'regressor__alpha': 10.050543165807229, 'regressor__fit_intercept': True})\n",
      "\n",
      "Weighted Quality Index + Optimized Linear Regression RMSE (Train): 17,424.57\n",
      "Weighted Quality Index + Optimized Linear Regression R² (Train): 0.9457\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Submission file 'bayesian_weighted_quality_test1.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  153664.994870\n",
      "1  1106  338830.701457\n",
      "2   414  103134.002587\n",
      "3   523  154867.006058\n",
      "4  1037  308118.876061\n"
     ]
    }
   ],
   "source": [
    "#try1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Real, Integer\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging & Quality Index ---\n",
    "\n",
    "# Helper function to create a general quality index\n",
    "def create_quality_index(df):\n",
    "    # Mapping for General Quality/Condition features (e.g., Ex, Gd, Ta, Fa, Po)\n",
    "    quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0, 'No': 0, 'NaN': 0}\n",
    "    \n",
    "    quality_cols = [\n",
    "        'ExteriorQuality', 'ExteriorCondition', 'KitchenQuality', 'HeatingQuality',\n",
    "        'BasementCondition', 'BasementExposure', 'LoungeQuality', 'ParkingQuality'\n",
    "    ]\n",
    "    \n",
    "    # Sum scores from available quality columns\n",
    "    df['TotalQualityScore'] = 0\n",
    "    \n",
    "    for col in quality_cols:\n",
    "        if col in df.columns:\n",
    "            # Map, fill NaNs, and add to total score\n",
    "            score = df[col].fillna('None').astype(str).str.upper().map(quality_map).fillna(0)\n",
    "            df['TotalQualityScore'] += score\n",
    "            # Drop individual quality columns to reduce noise/multicollinearity\n",
    "            df.drop(columns=[col], errors='ignore', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_train = create_quality_index(X_train)\n",
    "X_test = create_quality_index(X_test)\n",
    "print(\"\\nMerged quality/condition features into TotalQualityScore.\")\n",
    "\n",
    "# Merge Basement Features into Weighted Quality Score (Original)\n",
    "print(\"Merging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "# Merge Porch/Veranda Features (Original)\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    # Use 'OpenPorchArea' as the correct column name based on prior troubleshooting\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['OpenVerandaArea'].fillna(0) +\n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['OpenPorchArea', 'EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "             errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (including multicollinearity removal)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    'ParkingCapacity', \n",
    "    'GroundFloorArea', \n",
    "    'TotalRooms', \n",
    "    'UpperFloorArea', \n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering (Original) ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines (Original) ---\n",
    "\n",
    "# Numerical Transformer: Impute and Scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Bayesian Optimized Ridge) ---\n",
    "print(\"\\n--- 5. Model Training (Bayesian Optimized Ridge) ---\")\n",
    "\n",
    "poly_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # Note: Using degree=1 here to ensure a linear model for this test\n",
    "    ('poly', PolynomialFeatures(degree=1, include_bias=False)), \n",
    "    ('regressor', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "# Define search space for Bayesian Optimization\n",
    "search_space = {\n",
    "    # Only optimizing alpha and tolerance since degree is fixed at 1 (linear fit)\n",
    "    'regressor__alpha': Real(1e-6, 100, prior='log-uniform'), \n",
    "    'regressor__fit_intercept': Categorical([True, False]),\n",
    "}\n",
    "\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=poly_ridge_pipeline,\n",
    "    search_spaces=search_space,\n",
    "    n_iter=20, \n",
    "    cv=cv_strategy,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_fe, y_train_log)\n",
    "\n",
    "print(\"\\nBest Bayesian Hyperparameters:\")\n",
    "print(bayes_search.best_params_)\n",
    "\n",
    "# Train final model using best parameters\n",
    "final_model = bayes_search.best_estimator_\n",
    "\n",
    "# Evaluate optimized model on training data\n",
    "y_train_log_pred = final_model.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nWeighted Quality Index + Optimized Linear Regression RMSE (Train): {rmse_train:,.2f}\")\n",
    "print(f\"Weighted Quality Index + Optimized Linear Regression R² (Train): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = final_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'bayesian_weighted_quality_test1.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e508cdfd-d2e8-4bc9-944a-2da8406b9691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 66)\n",
      "Test data shape: (260, 66)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features (Total): 34\n",
      "Zero-imputed features: 13\n",
      "Median-imputed features: 21\n",
      "\n",
      "--- 5. Model Training (Bayesian Optimized Ridge with Targeted Imputation) ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best Bayesian Hyperparameters:\n",
      "OrderedDict({'regressor__alpha': 10.291536758968215, 'regressor__fit_intercept': True})\n",
      "\n",
      "Targeted Imputation + Optimized Linear Regression RMSE (Train): 16,704.59\n",
      "Targeted Imputation + Optimized Linear Regression R² (Train): 0.9501\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Submission file 'bayesian_targeted_imputation_test2.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151780.265931\n",
      "1  1106  332853.811716\n",
      "2   414  103730.854768\n",
      "3   523  152940.498454\n",
      "4  1037  316808.904602\n"
     ]
    }
   ],
   "source": [
    "#try2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Real\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging ---\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    # Merged porch area features\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['OpenVerandaArea'].fillna(0) + # Corrected name from previous version\n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['OpenPorchArea', 'EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "             errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (including multicollinearity removal)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    'ParkingCapacity', \n",
    "    'GroundFloorArea', \n",
    "    'TotalRooms', \n",
    "    'UpperFloorArea', \n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    # NOTE: The log transforms here are applied to features that WILL BE ZERO-IMPUTED later. \n",
    "    # This is fine since the missing values are handled by the manual imputation inside this function.\n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "# --- Define Feature Lists for Targeted Imputation ---\n",
    "all_numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Define features where NaN means 0 (Area, Length, etc.)\n",
    "zero_impute_features = [\n",
    "    'RoadAccessLength', 'TotalPorchArea', 'BasementTotalSF', 'BasementFinishedSF', \n",
    "    'FacadeArea', 'ParkingArea', 'TotalBasementScore', 'Fireplaces', 'LandArea',\n",
    "    'RoadAccessLength_Log', 'FacadeArea_Log', 'BasementTotalSF_Log', 'ParkingArea_Log', \n",
    "    'LandArea_Log'\n",
    "]\n",
    "# Ensure zero_impute_features only contains columns that actually exist\n",
    "zero_impute_features = [col for col in zero_impute_features if col in all_numerical_features]\n",
    "\n",
    "# The rest of the numerical features (where NaN is best filled by the median)\n",
    "general_numerical_features = [\n",
    "    col for col in all_numerical_features if col not in zero_impute_features\n",
    "]\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features (Total): {len(all_numerical_features)}\")\n",
    "print(f\"Zero-imputed features: {len(zero_impute_features)}\")\n",
    "print(f\"Median-imputed features: {len(general_numerical_features)}\")\n",
    "\n",
    "\n",
    "# --- 4. Preprocessing Pipelines (Targeted Imputation) ---\n",
    "\n",
    "# Pipeline 1: Zero Imputation and Scaling (for areas/lengths where NaN=0)\n",
    "zero_impute_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)), # FIX: Zero Imputation\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "# Pipeline 2: Median Imputation and Scaling (for general metrics)\n",
    "median_impute_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), # Original Median Imputation\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "# Categorical Transformer (Unchanged)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('zero_num', zero_impute_transformer, zero_impute_features), # Added Zero Imputation\n",
    "        ('median_num', median_impute_transformer, general_numerical_features), # Kept Median Imputation\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Bayesian Optimized Ridge) ---\n",
    "print(\"\\n--- 5. Model Training (Bayesian Optimized Ridge with Targeted Imputation) ---\")\n",
    "\n",
    "poly_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=1, include_bias=False)), # Linear Model\n",
    "    ('regressor', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "# Define search space for Bayesian Optimization\n",
    "search_space = {\n",
    "    'regressor__alpha': Real(1e-6, 100, prior='log-uniform'), \n",
    "    'regressor__fit_intercept': Categorical([True, False]),\n",
    "}\n",
    "\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=poly_ridge_pipeline,\n",
    "    search_spaces=search_space,\n",
    "    n_iter=20, \n",
    "    cv=cv_strategy,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_fe, y_train_log)\n",
    "\n",
    "print(\"\\nBest Bayesian Hyperparameters:\")\n",
    "print(bayes_search.best_params_)\n",
    "\n",
    "# Train final model using best parameters\n",
    "final_model = bayes_search.best_estimator_\n",
    "\n",
    "# Evaluate optimized model on training data\n",
    "y_train_log_pred = final_model.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nTargeted Imputation + Optimized Linear Regression RMSE (Train): {rmse_train:,.2f}\")\n",
    "print(f\"Targeted Imputation + Optimized Linear Regression R² (Train): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = final_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'bayesian_targeted_imputation_test2.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87bf142-c042-44e6-8dda-a813467e14e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 66)\n",
      "Test data shape: (260, 66)\n",
      "\n",
      "--- 3. Feature Engineering Complete ---\n",
      "Number of numerical features: 34\n",
      "Number of categorical features: 37\n",
      "Final training features shape: (1194, 71)\n",
      "\n",
      "--- 5. Model Training (Optimized Random Forest Regressor) ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best Random Forest Hyperparameters:\n",
      "OrderedDict({'regressor__max_depth': 15, 'regressor__max_features': 0.5, 'regressor__min_samples_leaf': 1, 'regressor__min_samples_split': 2, 'regressor__n_estimators': 618})\n",
      "\n",
      "Random Forest RMSE (Train): 9,477.91\n",
      "Random Forest R² (Train): 0.9839\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Submission file 'random_forest_optimized_test3.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  142185.771059\n",
      "1  1106  328331.360527\n",
      "2   414  112537.745793\n",
      "3   523  149700.994910\n",
      "4  1037  297588.894933\n"
     ]
    }
   ],
   "source": [
    "#try3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# NEW: Random Forest Regressor and Hyperparameter Optimization\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup (Unchanged) ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal (Unchanged) ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging (Unchanged) ---\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    # Corrected name for porch area\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['OpenVerandaArea'].fillna(0) + \n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['OpenPorchArea', 'EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "             errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (Unchanged)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    'ParkingCapacity', \n",
    "    'GroundFloorArea', \n",
    "    'TotalRooms', \n",
    "    'UpperFloorArea', \n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation (Unchanged) ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering (Unchanged) ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines (Simplified for Random Forest) ---\n",
    "\n",
    "# Numerical Transformer: Only Impute (Scaling is NOT needed for tree models)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # REMOVED: Scaling is not needed\n",
    "])\n",
    "\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0.0 # Force dense output for better compatibility\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Optimized Random Forest) ---\n",
    "print(\"\\n--- 5. Model Training (Optimized Random Forest Regressor) ---\")\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    # Random Forest is the best bagging method for this data type\n",
    "    ('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Define Bayesian search space for Random Forest (focus on complexity/regularization)\n",
    "search_space_rf = {\n",
    "    'regressor__n_estimators': Integer(200, 800), # Number of trees\n",
    "    'regressor__max_depth': Integer(5, 15),       # Max depth of each tree (crucial for controlling overfitting)\n",
    "    'regressor__min_samples_split': Integer(2, 10),\n",
    "    'regressor__min_samples_leaf': Integer(1, 5),\n",
    "    'regressor__max_features': Categorical(['sqrt', 0.5, 0.7]), # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_search_rf = BayesSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    search_spaces=search_space_rf,\n",
    "    n_iter=20, # Number of optimization iterations\n",
    "    cv=cv_strategy,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "bayes_search_rf.fit(X_train_fe, y_train_log)\n",
    "\n",
    "print(\"\\nBest Random Forest Hyperparameters:\")\n",
    "print(bayes_search_rf.best_params_)\n",
    "\n",
    "# Train final model using best parameters\n",
    "final_model = bayes_search_rf.best_estimator_\n",
    "\n",
    "# Evaluate optimized model on training data\n",
    "y_train_log_pred = final_model.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest RMSE (Train): {rmse_train:,.2f}\")\n",
    "print(f\"Random Forest R² (Train): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = final_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'random_forest_optimized_test3.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56aba545-7061-4e1f-95cb-5160508a895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 65)\n",
      "Test data shape: (260, 65)\n",
      "\n",
      "--- 3. Feature Engineering & Pruning Complete ---\n",
      "Number of numerical features (Pruned): 29\n",
      "Number of categorical features: 37\n",
      "Final training features shape: (1194, 66)\n",
      "\n",
      "--- 5. Model Training (Bayesian Optimized Ridge with Pruning) ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best Bayesian Hyperparameters:\n",
      "OrderedDict({'regressor__alpha': 10.0, 'regressor__fit_intercept': True})\n",
      "\n",
      "Multicollinearity Pruned + Optimized Linear Regression RMSE (Train): 17,151.80\n",
      "Multicollinearity Pruned + Optimized Linear Regression R² (Train): 0.9473\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Submission file 'bayesian_pruning_final.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151295.205690\n",
      "1  1106  333794.094236\n",
      "2   414  104401.764039\n",
      "3   523  151874.063653\n",
      "4  1037  313749.742518\n"
     ]
    }
   ],
   "source": [
    "#try4\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Real, Integer\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging ---\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['OpenVerandaArea'].fillna(0) + \n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['OpenVerandaArea', 'EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "             errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (Original Multicollinearity/Redundancy list)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    'ParkingCapacity', \n",
    "    'GroundFloorArea', \n",
    "    'TotalRooms', \n",
    "    'UpperFloorArea', \n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "# --- 3.5 Multicollinearity Pruning (NEW STEP) ---\n",
    "# Dropping the highly correlated numerical features (rho > 0.85) to stabilize the linear model.\n",
    "multicollinearity_drop_features = [\n",
    "    'BasementFinishedSF',   # Retain TotalBasementScore\n",
    "    'UsableArea',           # Retain QualityArea\n",
    "    'FullBaths',            # Retain TotalBathrooms\n",
    "    'RoadAccessLength'      # Retain RoadAccessLength_Log\n",
    "]\n",
    "X_train_fe = X_train_fe.drop(columns=multicollinearity_drop_features, errors='ignore')\n",
    "X_test_fe = X_test_fe.drop(columns=multicollinearity_drop_features, errors='ignore')\n",
    "\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering & Pruning Complete ---\")\n",
    "print(f\"Number of numerical features (Pruned): {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute and Scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Bayesian Optimized Ridge with Pruning) ---\n",
    "print(\"\\n--- 5. Model Training (Bayesian Optimized Ridge with Pruning) ---\")\n",
    "\n",
    "poly_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=1, include_bias=False)), # Linear Model (degree=1)\n",
    "    ('regressor', Ridge(random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define Bayesian search space\n",
    "search_space = {\n",
    "    'regressor__alpha': Real(1e-6, 1e1, prior='log-uniform'), \n",
    "    'regressor__fit_intercept': Categorical([True, False]),\n",
    "}\n",
    "\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=poly_ridge_pipeline,\n",
    "    search_spaces=search_space,\n",
    "    n_iter=20, # Number of optimization iterations\n",
    "    cv=cv_strategy,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_fe, y_train_log)\n",
    "\n",
    "print(\"\\nBest Bayesian Hyperparameters:\")\n",
    "print(bayes_search.best_params_)\n",
    "\n",
    "# Train final model using best parameters\n",
    "final_model = bayes_search.best_estimator_\n",
    "\n",
    "# Evaluate optimized model on training data\n",
    "y_train_log_pred = final_model.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nMulticollinearity Pruned + Optimized Linear Regression RMSE (Train): {rmse_train:,.2f}\")\n",
    "print(f\"Multicollinearity Pruned + Optimized Linear Regression R² (Train): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = final_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'bayesian_pruning_final.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371ab195-85df-480b-a229-05fd191b5666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 65)\n",
      "Test data shape: (260, 65)\n",
      "\n",
      "--- 3. Feature Engineering & Pruning Complete ---\n",
      "Number of numerical features (Pruned): 29\n",
      "Number of categorical features: 37\n",
      "Final training features shape: (1194, 66)\n",
      "\n",
      "--- 5. Model Training (Bayesian Optimized Ridge with Pruning) ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best Bayesian Hyperparameters:\n",
      "OrderedDict({'regressor__alpha': 10.0, 'regressor__fit_intercept': True})\n",
      "\n",
      "Multicollinearity Pruned + Optimized Linear Regression RMSE (Train): 17,151.80\n",
      "Multicollinearity Pruned + Optimized Linear Regression R² (Train): 0.9473\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Submission file 'bayesian_pruning_final.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151295.205690\n",
      "1  1106  333794.094236\n",
      "2   414  104401.764039\n",
      "3   523  151874.063653\n",
      "4  1037  313749.742518\n"
     ]
    }
   ],
   "source": [
    "#try5\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Real, Integer\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging ---\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['OpenVerandaArea'].fillna(0) + \n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['OpenVerandaArea', 'EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "             errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (Original Multicollinearity/Redundancy list)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    'ParkingCapacity', \n",
    "    'GroundFloorArea', \n",
    "    'TotalRooms', \n",
    "    'UpperFloorArea', \n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "# --- 3.5 Multicollinearity Pruning (NEW STEP) ---\n",
    "# Dropping the highly correlated numerical features (rho > 0.85) to stabilize the linear model.\n",
    "# Retain the engineered/log-transformed features where possible.\n",
    "multicollinearity_drop_features = [\n",
    "    'BasementFinishedSF',   # Retain TotalBasementScore\n",
    "    'UsableArea',           # Retain QualityArea\n",
    "    'FullBaths',            # Retain TotalBathrooms\n",
    "    'RoadAccessLength'      # Retain RoadAccessLength_Log\n",
    "]\n",
    "X_train_fe = X_train_fe.drop(columns=multicollinearity_drop_features, errors='ignore')\n",
    "X_test_fe = X_test_fe.drop(columns=multicollinearity_drop_features, errors='ignore')\n",
    "\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering & Pruning Complete ---\")\n",
    "print(f\"Number of numerical features (Pruned): {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute and Scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Bayesian Optimized Ridge with Pruning) ---\n",
    "print(\"\\n--- 5. Model Training (Bayesian Optimized Ridge with Pruning) ---\")\n",
    "\n",
    "poly_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=1, include_bias=False)), # Linear Model (degree=1)\n",
    "    ('regressor', Ridge(random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define Bayesian search space\n",
    "search_space = {\n",
    "    'regressor__alpha': Real(1e-6, 1e1, prior='log-uniform'), \n",
    "    'regressor__fit_intercept': Categorical([True, False]),\n",
    "}\n",
    "\n",
    "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=poly_ridge_pipeline,\n",
    "    search_spaces=search_space,\n",
    "    n_iter=20, # Number of optimization iterations\n",
    "    cv=cv_strategy,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_fe, y_train_log)\n",
    "\n",
    "print(\"\\nBest Bayesian Hyperparameters:\")\n",
    "print(bayes_search.best_params_)\n",
    "\n",
    "# Train final model using best parameters\n",
    "final_model = bayes_search.best_estimator_\n",
    "\n",
    "# Evaluate optimized model on training data\n",
    "y_train_log_pred = final_model.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nMulticollinearity Pruned + Optimized Linear Regression RMSE (Train): {rmse_train:,.2f}\")\n",
    "print(f\"Multicollinearity Pruned + Optimized Linear Regression R² (Train): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = final_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'bayesian_pruning_final.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41772afe-9cc3-4aa2-8176-c021d8744608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 65)\n",
      "Test data shape: (260, 65)\n",
      "\n",
      "--- 3. Feature Engineering & Pruning Complete ---\n",
      "Number of numerical features (Pruned): 29\n",
      "Number of categorical features: 37\n",
      "Final training features shape: (1194, 66)\n",
      "\n",
      "--- 5. Model Training (Bayesian Optimized Ridge with 75/25 Split) ---\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "\n",
      "Best Bayesian Hyperparameters:\n",
      "OrderedDict({'regressor__alpha': 10.0, 'regressor__fit_intercept': True})\n",
      "\n",
      "75/25 Split CV RMSE (Train): 17,151.80\n",
      "75/25 Split CV R² (Train): 0.9473\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Submission file 'bayesian_75_25_split_final.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151295.205690\n",
      "1  1106  333794.094236\n",
      "2   414  104401.764039\n",
      "3   523  151874.063653\n",
      "4  1037  313749.742518\n"
     ]
    }
   ],
   "source": [
    "#try6\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Real, Integer\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging ---\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    # Correcting column name to 'OpenVerandaArea'\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['OpenVerandaArea'].fillna(0) + \n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['OpenVerandaArea', 'EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "             errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (Original Multicollinearity/Redundancy list)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    'ParkingCapacity', \n",
    "    'GroundFloorArea', \n",
    "    'TotalRooms', \n",
    "    'UpperFloorArea', \n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "# --- 3.5 Multicollinearity Pruning ---\n",
    "multicollinearity_drop_features = [\n",
    "    'BasementFinishedSF',   \n",
    "    'UsableArea',           \n",
    "    'FullBaths',            \n",
    "    'RoadAccessLength'      \n",
    "]\n",
    "X_train_fe = X_train_fe.drop(columns=multicollinearity_drop_features, errors='ignore')\n",
    "X_test_fe = X_test_fe.drop(columns=multicollinearity_drop_features, errors='ignore')\n",
    "\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering & Pruning Complete ---\")\n",
    "print(f\"Number of numerical features (Pruned): {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute and Scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Bayesian Optimized Ridge with 75/25 Split) ---\n",
    "print(\"\\n--- 5. Model Training (Bayesian Optimized Ridge with 75/25 Split) ---\")\n",
    "\n",
    "poly_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=1, include_bias=False)), # Linear Model (degree=1)\n",
    "    ('regressor', Ridge(random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define Bayesian search space\n",
    "search_space = {\n",
    "    'regressor__alpha': Real(1e-6, 1e1, prior='log-uniform'), \n",
    "    'regressor__fit_intercept': Categorical([True, False]),\n",
    "}\n",
    "\n",
    "# CHANGE: Using n_splits=4 for KFold to achieve 75% training / 25% validation split\n",
    "cv_strategy = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=poly_ridge_pipeline,\n",
    "    search_spaces=search_space,\n",
    "    n_iter=20, # Number of optimization iterations\n",
    "    cv=cv_strategy, # Changed to 4 folds\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_fe, y_train_log)\n",
    "\n",
    "print(\"\\nBest Bayesian Hyperparameters:\")\n",
    "print(bayes_search.best_params_)\n",
    "\n",
    "# Train final model using best parameters\n",
    "final_model = bayes_search.best_estimator_\n",
    "\n",
    "# Evaluate optimized model on training data\n",
    "y_train_log_pred = final_model.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\n75/25 Split CV RMSE (Train): {rmse_train:,.2f}\")\n",
    "print(f\"75/25 Split CV R² (Train): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = final_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'bayesian_75_25_split_final.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be07d18-c064-4f2d-9856-ddf2b01a96c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading ---\n",
      "Loaded data from 'Hotel-Property-Value-Dataset/' folder.\n",
      "Initial training data shape: (1200, 80)\n",
      "Rows removed due to extreme outliers: 6\n",
      "\n",
      "Merging basement features...\n",
      "Merging porch features...\n",
      "Cleaned training data shape: (1194, 65)\n",
      "Test data shape: (260, 65)\n",
      "\n",
      "--- 3. Feature Engineering & Pruning Complete ---\n",
      "Number of numerical features (Pruned): 29\n",
      "Number of categorical features: 37\n",
      "Final training features shape: (1194, 66)\n",
      "\n",
      "--- 5. Model Training (Bayesian Optimized Ridge with 6 Folds) ---\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "Fitting 6 folds for each of 1 candidates, totalling 6 fits\n",
      "\n",
      "Best Bayesian Hyperparameters:\n",
      "OrderedDict({'regressor__alpha': 10.0, 'regressor__fit_intercept': True})\n",
      "\n",
      "6-Fold CV RMSE (Train): 17,151.80\n",
      "6-Fold CV R² (Train): 0.9473\n",
      "\n",
      "--- 6. Prediction & Submission ---\n",
      "Submission file 'bayesian_6_fold_final.csv' created with 260 predictions.\n",
      "First 5 test predictions:\n",
      "     Id     HotelValue\n",
      "0   893  151295.205690\n",
      "1  1106  333794.094236\n",
      "2   414  104401.764039\n",
      "3   523  151874.063653\n",
      "4  1037  313749.742518\n"
     ]
    }
   ],
   "source": [
    "#try7\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Real, Integer\n",
    "\n",
    "# Define file paths\n",
    "train_data_path = 'train.csv'\n",
    "test_data_path = 'test.csv'\n",
    "TARGET_COLUMN = 'HotelValue'\n",
    "\n",
    "# --- 1. Data Loading and Initial Setup ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "try:\n",
    "    df_train = pd.read_csv('Hotel-Property-Value-Dataset/train.csv')\n",
    "    df_test = pd.read_csv('Hotel-Property-Value-Dataset/test.csv')\n",
    "    print(\"Loaded data from 'Hotel-Property-Value-Dataset/' folder.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df_train = pd.read_csv(train_data_path)\n",
    "        df_test = pd.read_csv(test_data_path)\n",
    "        print(\"Loaded data from current directory.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Files not found. Ensure 'train.csv' and 'test.csv' are in the correct location.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        raise\n",
    "\n",
    "# Separate features and target before dropping/cleaning\n",
    "X_train_raw = df_train.drop(columns=[TARGET_COLUMN])\n",
    "y_train_raw = df_train[TARGET_COLUMN]\n",
    "X_test = df_test.copy()\n",
    "test_ids = X_test['Id']\n",
    "\n",
    "print(f\"Initial training data shape: {X_train_raw.shape}\")\n",
    "\n",
    "\n",
    "# --- 1.5 Outlier Removal ---\n",
    "initial_row_count = len(df_train)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "if 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "    outlier_mask &= ~((X_train_raw['OverallQuality'] < 5) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "X_train = X_train_raw[outlier_mask].copy()\n",
    "y_train = y_train_raw[outlier_mask].copy()\n",
    "test_ids_cleaned = X_test['Id'] \n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")\n",
    "\n",
    "\n",
    "# --- 1.6 Advanced Feature Merging ---\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "for df in [X_train, X_test]:\n",
    "    df['BasementFacilitySF1'] = df['BasementFacilitySF1'].fillna(0)\n",
    "    df['BasementFacilitySF2'] = df['BasementFacilitySF2'].fillna(0)\n",
    "    \n",
    "    df['Type1_Score'] = df['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    df['Type2_Score'] = df['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "    \n",
    "    df['TotalBasementScore'] = (df['Type1_Score'] * df['BasementFacilitySF1']) + (df['Type2_Score'] * df['BasementFacilitySF2'])\n",
    "    df['BasementFinishedSF'] = df['BasementFacilitySF1'] + df['BasementFacilitySF2']\n",
    "    \n",
    "    df['BasementAvgQuality'] = 0.0\n",
    "    mask_has_finished = df['BasementFinishedSF'] > 0\n",
    "    df.loc[mask_has_finished, 'BasementAvgQuality'] = (\n",
    "        df.loc[mask_has_finished, 'TotalBasementScore'] / df.loc[mask_has_finished, 'BasementFinishedSF']\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                     'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                     'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"Merging porch features...\")\n",
    "for df in [X_train, X_test]:\n",
    "    df['TotalPorchArea'] = (\n",
    "        df['OpenVerandaArea'].fillna(0) + \n",
    "        df['EnclosedVerandaArea'].fillna(0) + \n",
    "        df['SeasonalPorchArea'].fillna(0) + \n",
    "        df['ScreenPorchArea'].fillna(0)\n",
    "    )\n",
    "    df.drop(columns=['OpenVerandaArea', 'EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "             errors='ignore', inplace=True)\n",
    "\n",
    "# Columns to drop (Original Multicollinearity/Redundancy list)\n",
    "columns_to_drop = [\n",
    "    'Id', 'PoolQuality', 'BoundaryFence', 'ExtraFacility', 'ServiceLaneType', \n",
    "    'BasementHalfBaths', 'LowQualityArea',\n",
    "    'ParkingCapacity', \n",
    "    'GroundFloorArea', \n",
    "    'TotalRooms', \n",
    "    'UpperFloorArea', \n",
    "]\n",
    "X_train = X_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Target Transformation ---\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['HouseAge'] = df['YearSold'] - df['ConstructionYear']\n",
    "    \n",
    "    df['RenovationYear'] = df['RenovationYear'].fillna(df['ConstructionYear'])\n",
    "    df.loc[df['RenovationYear'] == 0, 'RenovationYear'] = df.loc[df['RenovationYear'] == 0, 'ConstructionYear']\n",
    "    \n",
    "    df['YearsSinceModification'] = df['YearSold'] - df[['ConstructionYear', 'RenovationYear']].max(axis=1)\n",
    "    \n",
    "    df['QualityArea'] = df['OverallQuality'] * df['UsableArea']\n",
    "    \n",
    "    if 'FullBaths' in df.columns and 'HalfBaths' in df.columns:\n",
    "        df['TotalBathrooms'] = df['FullBaths'] + (0.5 * df['HalfBaths'])\n",
    "    \n",
    "    for col in ['RoadAccessLength', 'LandArea', 'FacadeArea', 'BasementTotalSF', 'ParkingArea']:\n",
    "        if col in df.columns:\n",
    "            temp_df = df[col].fillna(0)\n",
    "            df[col + '_Log'] = np.log1p(temp_df)\n",
    "    \n",
    "    df = df.drop(columns=['ConstructionYear', 'RenovationYear', 'YearSold', 'MonthSold'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "X_train_fe = engineer_features(X_train)\n",
    "X_test_fe = engineer_features(X_test)\n",
    "\n",
    "# --- 3.5 Multicollinearity Pruning ---\n",
    "multicollinearity_drop_features = [\n",
    "    'BasementFinishedSF',   \n",
    "    'UsableArea',           \n",
    "    'FullBaths',            \n",
    "    'RoadAccessLength'      \n",
    "]\n",
    "X_train_fe = X_train_fe.drop(columns=multicollinearity_drop_features, errors='ignore')\n",
    "X_test_fe = X_test_fe.drop(columns=multicollinearity_drop_features, errors='ignore')\n",
    "\n",
    "\n",
    "numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\n--- 3. Feature Engineering & Pruning Complete ---\")\n",
    "print(f\"Number of numerical features (Pruned): {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"Final training features shape: {X_train_fe.shape}\")\n",
    "\n",
    "# --- 4. Preprocessing Pipelines ---\n",
    "\n",
    "# Numerical Transformer: Impute and Scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()), \n",
    "])\n",
    "\n",
    "# Categorical Transformer: Impute and One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. Model Training (Bayesian Optimized Ridge with 6 Folds) ---\n",
    "print(\"\\n--- 5. Model Training (Bayesian Optimized Ridge with 6 Folds) ---\")\n",
    "\n",
    "poly_ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=1, include_bias=False)), # Linear Model (degree=1)\n",
    "    ('regressor', Ridge(random_state=42, max_iter=10000))\n",
    "])\n",
    "\n",
    "# Define Bayesian search space\n",
    "search_space = {\n",
    "    'regressor__alpha': Real(1e-6, 1e1, prior='log-uniform'), \n",
    "    'regressor__fit_intercept': Categorical([True, False]),\n",
    "}\n",
    "\n",
    "# CHANGE: Using n_splits=6 for KFold to achieve 6-fold cross-validation\n",
    "cv_strategy = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=poly_ridge_pipeline,\n",
    "    search_spaces=search_space,\n",
    "    n_iter=20, \n",
    "    cv=cv_strategy, # Changed to 6 folds\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_fe, y_train_log)\n",
    "\n",
    "print(\"\\nBest Bayesian Hyperparameters:\")\n",
    "print(bayes_search.best_params_)\n",
    "\n",
    "# Train final model using best parameters\n",
    "final_model = bayes_search.best_estimator_\n",
    "\n",
    "# Evaluate optimized model on training data\n",
    "y_train_log_pred = final_model.predict(X_train_fe)\n",
    "y_train_pred = np.expm1(y_train_log_pred)\n",
    "y_train_pred[y_train_pred < 0] = 0\n",
    "\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\n6-Fold CV RMSE (Train): {rmse_train:,.2f}\")\n",
    "print(f\"6-Fold CV R² (Train): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# --- 6. Prediction and Submission File Creation ---\n",
    "print(\"\\n--- 6. Prediction & Submission ---\")\n",
    "y_test_log_pred = final_model.predict(X_test_fe)\n",
    "\n",
    "# Reverse log-transformation\n",
    "y_test_pred = np.expm1(y_test_log_pred)\n",
    "y_test_pred[y_test_pred < 0] = 0 \n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_ids_cleaned,\n",
    "    TARGET_COLUMN: y_test_pred\n",
    "})\n",
    "\n",
    "submission_filename = 'bayesian_6_fold_final.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_filename}' created with {len(submission_df)} predictions.\")\n",
    "print(\"First 5 test predictions:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
